{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickEsColR/MachineLearningV/blob/train/taller/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Redes Neuronales Tradicinales (MLP)\n",
        "2. Red Convolucional (CNN) adaptada a series temporales\n",
        "3. Red Neuronal Recurrente (RNN) básica.\n",
        "4. Modelo con LSTMs\n",
        "5. Modelo con GRUs"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "wKLm80Huf2vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.  Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.   Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta # Importing the required modules datetime and timedelta\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "import copy\n",
        "from typing import List, Tuple, Type, Any, Dict, Union\n",
        "import sys"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output\n",
        "!rm -rf output.zip\n",
        "!mkdir output"
      ],
      "metadata": {
        "id": "KYNyIrPovvUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1  Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el último día de cada semana como índice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['fecha'] = train_df['anio'].astype(str) + train_df['semana'].astype(str).str.zfill(2)\n",
        "test_df['fecha'] = test_df['anio'].astype(str) + test_df['semana'].astype(str).str.zfill(2)\n",
        "train_df.sort_values(by=['fecha','id_bar'])\n",
        "test_df.sort_values(by=['fecha','id_bar'])"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Particionamos el dataset en entrenamiento hasta el año 2020 y validación el 2021"
      ],
      "metadata": {
        "id": "6N21eCvZdiJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir conjunto de entrenamiento en train y validation\n",
        "train_df_full = train_df.copy()\n",
        "train_df = train_df_full[train_df_full.index.year <= 2020].copy()\n",
        "val_df = train_df_full[train_df_full.index.year >= 2021].copy()"
      ],
      "metadata": {
        "id": "rtoYCc5_ZtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Selección de Características\n",
        "Definimos las características de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las características disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina','lluvia_mean', 'temperatura_mean','temperatura_max']  # Selección basada en correlaciones\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Estandarizar\n"
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos las características y el objetivo usando StandardScaler. Identificamos las características numéricas (excluyendo id, id_bar y dengue):\n",
        "\n",
        "Características: ESTRATO, area_barrio, concentraciones, vivienda, equipesado, sumideros, maquina, lluvia_mean, lluvia_var, lluvia_max, lluvia_min, temperatura_mean, temperatura_var, temperatura_max, temperatura_min. Ajustamos escaladores por separado para características y objetivo:"
      ],
      "metadata": {
        "id": "3kTHM_xFSOuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se excluyeron variables como lluvia_var, lluvia_max, temperatura_var, y temperatura_min debido a sus altas correlaciones (e.g., lluvia_var y lluvia_mean: 0.82), para reducir redundancia y mejorar la estabilidad de los modelos."
      ],
      "metadata": {
        "id": "oJpWlJ8kZ9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización\n",
        "# scaler_features = MinMaxScaler()\n",
        "scaler_features = StandardScaler()\n",
        "train_df[features] = scaler_features.fit_transform(train_df[features])\n",
        "val_df[features] = scaler_features.transform(val_df[features])\n",
        "test_df[features] = scaler_features.transform(test_df[features])\n",
        "\n",
        "# scaler_target = StandardScaler()\n",
        "# scaler_target = MinMaxScaler()\n",
        "# train_df[target] = scaler_target.fit_transform(train_df[[target]])\n",
        "# val_df[target] = scaler_target.transform(val_df[[target]])\n",
        "# train_df[target] = np.log1p(train_df[[target]])\n",
        "# val_df[target] = np.log1p(val_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard scaler para las features y el target sin transformar fue la que dio mejores resultdos"
      ],
      "metadata": {
        "id": "cTRaQFgq2cSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipación, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df: pd.DataFrame,\n",
        "                     window_size: int,\n",
        "                     horizon: int,\n",
        "                     features: List[str],\n",
        "                     target: str,\n",
        "                     group_column: str) -> Tuple[List[np.ndarray], List[float]]:\n",
        "    \"\"\"\n",
        "    Creates sequences and labels for time series forecasting.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame containing the time series data.\n",
        "        window_size: The size of the rolling window.\n",
        "        horizon: The forecasting horizon.\n",
        "        features: A list of column names representing the input features.\n",
        "        target: The column name representing the target variable.\n",
        "        group_column: The column name to group the data by (e.g., 'id_bar').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the sequences (a list of NumPy arrays) and the labels (a list of floats).\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    ids = []\n",
        "\n",
        "    groups = df.groupby(group_column)\n",
        "    for _, group in groups:\n",
        "        group = group.sort_values(by=['id_bar', 'fecha'])\n",
        "        for i in range(len(group) - window_size - horizon + 1):\n",
        "            X = group.iloc[i:i + window_size][features].values\n",
        "            y = group.iloc[i + window_size + horizon - 1][target]\n",
        "            ids.append(group.iloc[i + window_size + horizon - 1]['id'])\n",
        "            sequences.append(X)\n",
        "            labels.append(y)\n",
        "\n",
        "    return sequences, labels,ids\n",
        "\n",
        "train_sequences, train_labels, train_ids = create_sequences(train_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "val_sequences, val_labels, val_ids = create_sequences(val_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validación."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        sequences: A list of NumPy arrays representing the input sequences.\n",
        "        labels: A list of floats representing the corresponding target values.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences: List[np.ndarray], labels: List[float]):\n",
        "        \"\"\"Initializes the DengueDataset with sequences and labels.\"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns the input sequence and label for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the input sequence (as a PyTorch tensor)\n",
        "            and the corresponding label (as a PyTorch tensor).\n",
        "        \"\"\"\n",
        "        X = self.sequences[idx]\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueTestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for Dengue forecasting for test data.\n",
        "\n",
        "    Args:\n",
        "        sequences: A list of NumPy arrays representing the input sequences for test data.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences: List[np.ndarray]):\n",
        "        \"\"\"Initializes the DengueTestDataset with sequences.\"\"\"\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the input sequence for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            The input sequence (as a PyTorch tensor).\n",
        "        \"\"\"\n",
        "        X = self.sequences[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EPVVoaE_diEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DengueDataset(train_sequences, train_labels)\n",
        "val_dataset = DengueDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "6_RtD605chV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.  Implementación de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Modelo MLP\n",
        "Un Perceptrón Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Multilayer Perceptron (MLP) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden layers.\n",
        "        layer_dim: The number of hidden layers (default: 1).\n",
        "        output_dim: The dimensionality of the output (default: 1).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim:int = 1, output_dim: int = 1, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the MLPModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(MLPModel, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim * config['WINDOW_SIZE'], hidden_dim)) # multiply window_size since we need to receive a tensor like (batch,window_size, features) and convert to (batch,window_size*features) to correctly output the prediction\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        for _ in range(layer_dim - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the MLP model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "para usar la arquitectura MLP junto con las otras en la busqueda bayesiana y no tener que modificar el dataset cuando se quiere usar MLP. Los ajustes que se hacen en la clase *MLPModel* son:\n",
        "\n",
        "-  multiplicar *input_dim* con la ventana definida para enviar la cantidad de caracteristicas como dimension de entrada al igual que se hace con los otros modelos, pero permitiendole al modelo recibir las caracteristicas aplanadas por la ventana.\n",
        "-  en el *forward pass* se aplana la entrada pero permaneciendo el batch. Esto porque MLP solo recibe datos en una ."
      ],
      "metadata": {
        "id": "abPyTctaOvR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A 1D Convolutional Neural Network (CNN) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden layers.\n",
        "        layer_dim: The number of hidden layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim:int = 1, output_dim: int = 1, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the CNNModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1))\n",
        "        layers.append(nn.ReLU())\n",
        "        for _ in range(layer_dim - 1):\n",
        "            layers.append(nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the CNN model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, window_size)\n",
        "        x = self.model(x)\n",
        "        x = self.pool(x).squeeze(-1) # La salida es (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r0LS148fv7q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el forward pass se acomoda la entrada para recibir las caracteristicas como el canal de entrada."
      ],
      "metadata": {
        "id": "aTB2SCwXfPEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Modelo RNN Básico\n"
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic Recurrent Neural Network (RNN) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of RNN layers (default: 1).\n",
        "        output_dim: The dimensionality of the output (default: 1).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int = 1, output_dim: int = 1, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the RNNModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the RNN model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uXl5Db3cxCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Long Short-Term Memory (LSTM) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of LSTM layers (default: 1).\n",
        "        output_dim: The dimensionality of the output (default: 1).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int = 1, output_dim: int = 1, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the LSTMModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Gated Recurrent Unit (GRU) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of GRU layers (default: 1).\n",
        "        output_dim: The dimensionality of the output (default: 1).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int = 1, output_dim: int = 1, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the GRUModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the GRU model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Función de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model: nn.Module,\n",
        "                train_loader: DataLoader,\n",
        "                epochs: int,\n",
        "                optimizer: optim.Optimizer,\n",
        "                criterion: nn.Module,\n",
        "                device: torch.device,\n",
        "                val_loader: DataLoader = None,\n",
        "                patience: int = 10) -> Tuple[List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Trains a PyTorch model and returns the training and validation losses.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        train_loader: The DataLoader for the training data.\n",
        "        val_loader: The DataLoader for the validation data. If None, no validation is performed.\n",
        "        epochs: The number of epochs to train for.\n",
        "        optimizer: The optimizer to use.\n",
        "        criterion: The loss function to use.\n",
        "        device: The device to train on (e.g., 'cpu' or 'cuda').\n",
        "        patience: The number of epochs to wait for improvement before stopping. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the training losses and validation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if val_loader is None:\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}')\n",
        "          if train_loss < best_loss:\n",
        "            best_loss = train_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            epochs_without_improvement = 0\n",
        "          else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              break\n",
        "        else:\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "          model.eval()\n",
        "          val_loss = 0\n",
        "          with torch.no_grad():\n",
        "              for X, y in val_loader:\n",
        "                  X, y = X.to(device), y.to(device)\n",
        "                  output = model(X)\n",
        "                  loss = criterion(output, y)\n",
        "                  val_loss += loss.item()\n",
        "          val_loss /= len(val_loader)\n",
        "          val_losses.append(val_loss)\n",
        "          if val_loss < best_loss:\n",
        "              best_loss = val_loss\n",
        "              best_model = copy.deepcopy(model)\n",
        "              epochs_without_improvement = 0\n",
        "          else:\n",
        "              epochs_without_improvement += 1\n",
        "              if epochs_without_improvement >= patience:\n",
        "                  print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break\n",
        "\n",
        "    model = best_model\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Función de Evaluación"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model: torch.nn.Module,\n",
        "                   val_loader: torch.utils.data.DataLoader,\n",
        "                   device: torch.device,\n",
        "                   scaler_target: MinMaxScaler | StandardScaler) -> Tuple[float, float, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluates a PyTorch model on a validation set and returns evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to evaluate.\n",
        "        val_loader: The DataLoader for the validation data.\n",
        "        device: The device to evaluate on (e.g., 'cpu' or 'cuda').\n",
        "        scaler_target: The scaler used to normalize the target variable.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the mean absolute error (MAE), mean squared error (MSE),\n",
        "        root mean squared error (RMSE), predicted values, and actual values.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(y.cpu().numpy())\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, predictions, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Gráficos\n",
        "Generamos gráficos de pérdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses: List[float], val_losses: List[float]) -> None:\n",
        "    \"\"\"\n",
        "    Generates a plot of training and validation losses.\n",
        "\n",
        "    Args:\n",
        "        train_losses: A list of training losses for each epoch.\n",
        "        val_losses: A list of validation losses for each epoch.\n",
        "    \"\"\"\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals: np.ndarray, predictions: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Generates a plot of actual vs predicted values.\n",
        "\n",
        "    Args:\n",
        "        actuals: A NumPy array of actual values.\n",
        "        predictions: A NumPy array of predicted values.\n",
        "    \"\"\"\n",
        "    plt.plot(actuals, label='Actual')\n",
        "    plt.plot(predictions, label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.legend()\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Entrenar Modelos con optimización bayesiana"
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_optimization(train_loader: DataLoader, val_loader: DataLoader, space: Dict[str, Any], max_evals: int = 50, device: Union[str, torch.device] = 'cpu') -> Dict[str, Any]:\n",
        "\n",
        "    \"\"\"\n",
        "    Performs Bayesian optimization for hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        train_loader: The DataLoader for the training data.\n",
        "        val_loader: The DataLoader for the validation data.\n",
        "        space: The search space for hyperparameters.\n",
        "        max_evals: The maximum number of evaluations\n",
        "        device: The device to train on (e.g., 'cpu' or 'cuda'). Default is 'cpu'.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the best hyperparameters and the best model.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(params):\n",
        "        \"\"\"\n",
        "        Objective function for Bayesian optimization.\n",
        "\n",
        "        Args:\n",
        "            params: The hyperparameters to optimize.\n",
        "\n",
        "        Returns:\n",
        "            The negative validation loss.\n",
        "        \"\"\"\n",
        "        model = params['model'](input_dim=len(features), hidden_dim=params['hidden_dim'], dropout_rate=params['dropout_rate'])\n",
        "\n",
        "        criterion = params['loss_fn']\n",
        "        optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
        "        train_losses, val_losses = train_model(model=model, train_loader=train_loader, val_loader=val_loader, epochs=params['epochs'], optimizer=optimizer, criterion=criterion, device=device,patience=params['patience'])\n",
        "\n",
        "        return {'loss': val_losses[-1], 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "\n",
        "    # Get the best model from the trials\n",
        "    best_trial = trials.best_trial\n",
        "    best_model = best_trial['result']['model'] # Get the best model\n",
        "\n",
        "\n",
        "    return {'best_params': best, 'best_model': best_model}"
      ],
      "metadata": {
        "id": "qTvtQcr9ddwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparametros del espacio de búsqueda"
      ],
      "metadata": {
        "id": "e07rplY-Adsq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "un hidden_dim o layer_dim muy grande producen un desvanecimiento del gradiente, dando predicciones de una constante.\n",
        "Se encontro mejores resultados entre 1 y 2 capas, con una dimensionalidad maxima de 32 y pocas epocas."
      ],
      "metadata": {
        "id": "lTIN8ka-WIxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante los diferentes experimentos, en la mayoria de los casos la funcion *PoissonNLLLoss* y el optimizador *SGD* diron los mejores resultados. El mejor learning rate fue cercano a 1e-4, se recomienda fijarlo para reducir el espacio de busqueda."
      ],
      "metadata": {
        "id": "GSWw2oU0gapu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_choices = [10, 20, 30, 40, 50]\n",
        "patience_choices = [10, 50]\n",
        "hidden_dim_choices = [8, 12, 16, 24, 32]\n",
        "layer_dim_choices = [1, 2]\n",
        "optimizer_choices = [optim.Adam, optim.RMSprop, optim.SGD]\n",
        "loss_fn_choices = [nn.MSELoss(), nn.PoissonNLLLoss()]\n",
        "model_choice = [MLPModel, CNNModel, RNNModel, LSTMModel, GRUModel]\n",
        "max_evals_bayesian_search = 150"
      ],
      "metadata": {
        "id": "l32P0EC2qHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space = {\n",
        "    'model': hp.choice('model', model_choice),\n",
        "    'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "    'layer_dim': hp.choice('layer_dim', layer_dim_choices),\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0, 0.7),\n",
        "\n",
        "    'epochs': hp.choice('epochs', epochs_choices),\n",
        "    'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "    'lr': hp.loguniform('lr', np.log(1e-4), np.log(1e-2)),\n",
        "    'patience': hp.choice('patience',patience_choices),\n",
        "    'loss_fn': hp.choice('loss_fn',loss_fn_choices)\n",
        "}"
      ],
      "metadata": {
        "id": "GQWy3jcjNvgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = bayesian_optimization(train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k1972LeeeOJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejores hiperpárametros"
      ],
      "metadata": {
        "id": "i_FI_b02AmAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mejores parámetros:\")\n",
        "print(f\"Model: {model_choice[best_model['best_params']['model']]}\")\n",
        "print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\")\n",
        "print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\")\n",
        "print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\")\n",
        "print(\"--------------------------------------------------------------\")\n",
        "print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\")\n",
        "print(f\"patience: {patience_choices[best_model['best_params']['patience']]}\")\n",
        "print(f\"lr: {best_model['best_params']['lr']}\")\n",
        "print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\")\n",
        "print(f\"loss: {loss_fn_choices[best_model['best_params']['loss_fn']]}\")\n",
        "print(\"Mejor modelo:\", best_model['best_model'])"
      ],
      "metadata": {
        "id": "BfbFkN59Mks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output/params.txt', 'w') as f:\n",
        "    print(\"Mejores parámetros:\", file=f)\n",
        "    print(f\"Model: {model_choice[best_model['best_params']['model']]}\", file=f)\n",
        "    print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\", file=f)\n",
        "    print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\", file=f)\n",
        "    print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\", file=f)\n",
        "    print(\"--------------------------------------------------------------\",file=f)\n",
        "    print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\", file=f)\n",
        "    print(f\"patience: {patience_choices[best_model['best_params']['patience']]}\", file=f)\n",
        "    print(f\"lr: {space['lr']}\", file=f)\n",
        "    print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\", file=f)\n",
        "    print(f\"loss: {loss_fn_choices[best_model['best_params']['loss_fn']]}\", file=f)\n",
        "    print(\"Mejor modelo:\", best_model['best_model'], file=f)\n",
        "    print(\"--------------------------------------------------------------\",file=f)\n",
        "    print(f\"Window: {config['WINDOW_SIZE']}\",file=f)\n",
        "    print(f\"Horizon: {config['HORIZON']}\",file=f)\n",
        "    print(f\"Features: {features}\",file=f)\n",
        "    print(f\"Model choices: {model_choice}\",file=f)\n",
        "    print(f\"adicional info\",file=f)"
      ],
      "metadata": {
        "id": "pIw8QAxJuzlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Predicción en el Test Set"
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Re entrenar modelo final"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una copia del modelo para re entrenar con todos los datos disponibles."
      ],
      "metadata": {
        "id": "wPztJe2txYd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_trained = copy.deepcopy(best_model['best_model']).to(DEVICE)"
      ],
      "metadata": {
        "id": "YDWNB-DX7m9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos los datos para generar la secuencia como la espera el modelo, creando el dataset y el dataloader"
      ],
      "metadata": {
        "id": "bnAtXBL8xgTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "train_df_full[features] = scaler_features.transform(train_df_full[features])\n",
        "# train_df_full[target] = scaler_target.transform(train_df_full[[target]])\n",
        "\n",
        "# 2.\n",
        "train_sequences_full, train_labels_full, full_ids = create_sequences(train_df_full, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "\n",
        "# 3.\n",
        "train_dataset_full = DengueDataset(train_sequences_full, train_labels_full)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "xXizf831_oF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos nuevamente el modelo usando los mejores hiperparametros y basandonos en el mejor modelo"
      ],
      "metadata": {
        "id": "30fiv9RzxtoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🔁 Reentrenando con todo el dataset (train + val)...\\n\")\n",
        "criterion = loss_fn_choices[best_model['best_params']['loss_fn']]\n",
        "optimizer = optimizer_choices[best_model['best_params']['optimizer']](best_model_trained.parameters(), lr=best_model['best_params']['lr'])\n",
        "train_losses, val_losses = train_model(best_model_trained, train_loader_full,  epochs=epochs_choices[best_model['best_params']['epochs']], optimizer=optimizer, criterion=criterion, device=DEVICE, patience=patience_choices[best_model['best_params']['patience']])"
      ],
      "metadata": {
        "id": "mwTir67aBfXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df_full, test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = []\n",
        "ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(config[\"WINDOW_SIZE\"])\n",
        "    if len(prev_dates) == config[\"WINDOW_SIZE\"]:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        ids.append(row['id'])"
      ],
      "metadata": {
        "id": "GgynaqcgRe0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "h6XEwrRWQWi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  Submission"
      ],
      "metadata": {
        "id": "HJO3iLJO0klb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero preparamos el dataset de prueba"
      ],
      "metadata": {
        "id": "7QfmOEyM1hZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = DengueTestDataset(test_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generamos la predicción con el mejor modelo re-entrenado"
      ],
      "metadata": {
        "id": "pg61SRq81jxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecasting(modelo: Union[torch.nn.Module], dataloader: DataLoader) -> np.ndarray:\n",
        "    \"\"\"Generates dengue case predictions using the provided model and dataloader.\n",
        "\n",
        "    Args:\n",
        "        modelo: The trained PyTorch model used for forecasting.\n",
        "        dataloader: The DataLoader containing the test dataset.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of predicted dengue cases.\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:  # Iterate through batches from the dataloader\n",
        "            if isinstance(batch, list) or isinstance(batch, tuple):  # Check if batch is a list or tuple\n",
        "                X = batch[0]  # Get the input data (assuming it's the first element)\n",
        "            else:\n",
        "                X = batch  # Otherwise, assume batch is the input data directly\n",
        "            X = X.to(DEVICE)\n",
        "            output = modelo(X)\n",
        "            #output = torch.expm1(output)\n",
        "            #output = torch.round(output)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions).flatten()\n",
        "    # predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "XuHrD3NkLIST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = forecasting(best_model_trained, test_loader)\n",
        "\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('output/submission_retrain.csv', index=False)\n",
        "print(f'Submission guardado en submission_retrain.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "AzG1Xk47b3Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos la predicción con el mejor modelo sin re-entrenar"
      ],
      "metadata": {
        "id": "PeYQdy8n1ysI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = forecasting(best_model['best_model'], test_loader)\n",
        "\n",
        "# Prepare submission\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('output/submission.csv', index=False)\n",
        "print(f'Submission guardado en submission.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "upekEBDp16LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.  Grafica de predicción contra validación"
      ],
      "metadata": {
        "id": "wc2evmQGT4gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = forecasting(best_model['best_model'], val_loader)\n",
        "\n",
        "df_validation = pd.DataFrame({'id': val_ids, 'dengue': predictions})\n",
        "val_df = val_df.sort_values(by=['id_bar', 'fecha'])"
      ],
      "metadata": {
        "id": "jRd3PNSp801E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se guardan las graficas por barrio de la predicción del modelo contra el dataset de validación"
      ],
      "metadata": {
        "id": "kXI4WgFliTH7"
      }
    },
    {
      "source": [
        "# Extract neighborhood IDs from the 'id' column\n",
        "df_validation['id_bar'] = df_validation['id'].str.split('_').str[0].astype(int)\n",
        "df_validation['year'] = df_validation['id'].str.split('_').str[1].astype(int)\n",
        "df_validation['week'] = df_validation['id'].str.split('_').str[2].astype(int)\n",
        "\n",
        "\n",
        "# Group by neighborhood ID\n",
        "for id_bar, group in df_validation.groupby('id_bar'):\n",
        "    # Create a new figure for each neighborhood\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    # Plot 'dengue' values for the current neighborhood\n",
        "    plt.plot(group['week'].values, group['dengue'].values, label=f'Neighborhood {id_bar}')\n",
        "    # Filter true values from val_df for the current neighborhood and plot\n",
        "    true_values = val_df[(val_df['id_bar'] == id_bar)]['dengue'].values\n",
        "    plt.plot(true_values, label=f'Neighborhood {id_bar} (True)')\n",
        "\n",
        "    # Set title and labels\n",
        "    plt.title(f'Dengue Cases for Neighborhood {id_bar}')\n",
        "    plt.xlabel('Week')  # Assuming the x-axis represents weeks\n",
        "    plt.ylabel('Dengue Cases')\n",
        "\n",
        "    # Remove top and right spines\n",
        "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "\n",
        "    # Add legend\n",
        "    plt.legend()\n",
        "\n",
        "    # Save the plot\n",
        "    plt.savefig(f'output/val_plots/neighborhood_{id_bar}.png')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "-fDdOLKu_IRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9. Guardar modelo"
      ],
      "metadata": {
        "id": "YRIWpxjD1ZVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se guarda el mejor modelo."
      ],
      "metadata": {
        "id": "YP9JSSnSi5cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model_trained.state_dict(), 'output/best_model_retrained.pth')\n",
        "torch.save(best_model['best_model'].state_dict(), 'output/best_model.pth')"
      ],
      "metadata": {
        "id": "RxEv4ryo1ccP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output/model_summary.txt', 'w') as f:\n",
        "    print(\"Model Summary:\", file=f)\n",
        "    try:\n",
        "        print(summary(best_model_trained, (1, len(features))), file=f)\n",
        "    except:\n",
        "        try:\n",
        "            print(summary(best_model_trained, (1, len(features) * config['WINDOW_SIZE'])), file=f)\n",
        "        except:\n",
        "            print(\"Error al generar el summary\", file=f)"
      ],
      "metadata": {
        "id": "_Dv3Akvr4oVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se comprimen los diferentes archivos generados durante el experimento para una fácil descarga."
      ],
      "metadata": {
        "id": "_obkyD54iqXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_!zip -r output.zip output"
      ],
      "metadata": {
        "id": "rLsFcjeJwLuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}