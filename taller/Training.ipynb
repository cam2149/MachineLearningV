{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickEsColR/MachineLearningV/blob/train/taller/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicol치s Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situaci칩n:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipaci칩n de brotes es crucial para implementar medidas preventivas y reducir la propagaci칩n de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos hist칩rica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipaci칩n.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud p칰blica tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Redes Neuronales Tradicinales (MLP)\n",
        "2. Red Convolucional (CNN) adaptada a series temporales\n",
        "3. Red Neuronal Recurrente (RNN) b치sica.\n",
        "4. Modelo con LSTMs\n",
        "5. Modelo con GRUs"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripci칩n**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador 칰nico del barrio                                                                      |\n",
        "| anio                 | A침o de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioecon칩mico del barrio                                                                   |\n",
        "| area_barrio          | 츼rea del barrio en km                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervenci칩n a lugares de concentraci칩n humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisi칩n y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia m치xima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia m칤nima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura m치xima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura m칤nima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "wKLm80Huf2vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicaci칩n correcta despu칠s de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas l칤neas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta # Importing the required modules datetime and timedelta\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "import copy"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Configuraci칩n Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraci칩n del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el 칰ltimo d칤a de cada semana como 칤ndice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar columna 'fecha' (칰ltimo d칤a de la semana, domingo)\n",
        "def last_day_of_week(year, week):\n",
        "    first_day = datetime.strptime(f'{year} {week} 1', \"%Y %W %w\")\n",
        "    days_ahead = 6 - first_day.weekday()\n",
        "    last_day = first_day + timedelta(days=days_ahead)\n",
        "    return last_day\n",
        "\n",
        "train_df['fecha'] = train_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)\n",
        "test_df['fecha'] = test_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer 'fecha' como 칤ndice\n",
        "train_df.set_index('fecha', inplace=True)\n",
        "test_df.set_index('fecha', inplace=True)"
      ],
      "metadata": {
        "id": "3eyFu1UUZdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar columnas innecesarias\n",
        "#train_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)\n",
        "#test_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gLoH_lheZqEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Particionamos el dataset en entrenamiento hasta el a침o 2020 y validaci칩n el 2021"
      ],
      "metadata": {
        "id": "6N21eCvZdiJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir conjunto de entrenamiento en train y validation\n",
        "train_df_full = train_df.copy()\n",
        "train_df = train_df_full[train_df_full.index.year <= 2020].copy()\n",
        "val_df = train_df_full[train_df_full.index.year >= 2021].copy()"
      ],
      "metadata": {
        "id": "rtoYCc5_ZtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Selecci칩n de Caracter칤sticas\n",
        "Definimos las caracter칤sticas de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las caracter칤sticas disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir caracter칤sticas (excluyendo variables altamente correlacionadas)\n",
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
        "            'lluvia_mean', 'temperatura_mean', 'temperatura_max']  # Selecci칩n basada en correlaciones\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Normalizaci칩n\n"
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos las caracter칤sticas y el objetivo usando StandardScaler. Identificamos las caracter칤sticas num칠ricas (excluyendo id, id_bar y dengue):\n",
        "\n",
        "Caracter칤sticas: ESTRATO, area_barrio, concentraciones, vivienda, equipesado, sumideros, maquina, lluvia_mean, lluvia_var, lluvia_max, lluvia_min, temperatura_mean, temperatura_var, temperatura_max, temperatura_min. Ajustamos escaladores por separado para caracter칤sticas y objetivo:"
      ],
      "metadata": {
        "id": "3kTHM_xFSOuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se excluyeron variables como lluvia_var, lluvia_max, temperatura_var, y temperatura_min debido a sus altas correlaciones (e.g., lluvia_var y lluvia_mean: 0.82), para reducir redundancia y mejorar la estabilidad de los modelos."
      ],
      "metadata": {
        "id": "oJpWlJ8kZ9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizaci칩n\n",
        "scaler_features = StandardScaler()\n",
        "train_df[features] = scaler_features.fit_transform(train_df[features])\n",
        "val_df[features] = scaler_features.transform(val_df[features])\n",
        "test_df[features] = scaler_features.transform(test_df[features])\n",
        "\n",
        "scaler_target = StandardScaler()\n",
        "train_df[target] = scaler_target.fit_transform(train_df[[target]])\n",
        "val_df[target] = scaler_target.transform(val_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipaci칩n, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, window_size, horizon, features, target, group_column):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    groups = df.groupby(group_column)\n",
        "    for _, group in groups:\n",
        "        group = group.sort_index()\n",
        "        for i in range(len(group) - window_size - horizon + 1):\n",
        "            X = group.iloc[i:i + window_size][features].values\n",
        "            y = group.iloc[i + window_size + horizon - 1][target]\n",
        "            sequences.append(X)\n",
        "            labels.append(y)\n",
        "    return sequences, labels\n",
        "\n",
        "train_sequences, train_labels = create_sequences(train_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "val_sequences, val_labels = create_sequences(val_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validaci칩n."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueTestDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EPVVoaE_diEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DengueDataset(train_sequences, train_labels)\n",
        "val_dataset = DengueDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "6_RtD605chV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implementaci칩n de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Modelo MLP\n",
        "Un Perceptr칩n Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, window_size)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1) # La salida es (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r0LS148fv7q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3 Modelo RNN B치sico\n",
        "Implementaci칩n proporcionada con estados iniciales definidos."
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uXl5Db3cxCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Entrenamiento y Evaluaci칩n"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Funci칩n de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                output = model(X)\n",
        "                loss = criterion(output, y)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:  # Imprimir solo cada 10 칠pocas\n",
        "          print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Funci칩n de Evaluaci칩n"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, device, scaler_target):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(y.cpu().numpy())\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, predictions, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Gr치ficos\n",
        "Generamos gr치ficos de p칠rdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals, predictions):\n",
        "    plt.plot(actuals, label='Actual')\n",
        "    plt.plot(predictions, label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.legend()\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Entrenar Modelos con optimizaci칩n bayesiana"
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_optimization(model_class, train_dataset, val_dataset, space, max_evals=50, device='cpu'):\n",
        "    \"\"\"\n",
        "    Realiza la optimizaci칩n bayesiana de los hiperpar치metros de un modelo.\n",
        "\n",
        "    Args:\n",
        "        model_class: La clase del modelo a optimizar.\n",
        "        train_dataset: El conjunto de datos de entrenamiento.\n",
        "        val_dataset: El conjunto de datos de validaci칩n.\n",
        "        space: El espacio de b칰squeda de hiperpar치metros.\n",
        "        max_evals: El n칰mero m치ximo de evaluaciones.\n",
        "        device: El dispositivo a utilizar ('cpu' o 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        Un diccionario con los mejores hiperpar치metros y el mejor modelo.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(params):\n",
        "        \"\"\"\n",
        "        Funci칩n objetivo para la optimizaci칩n bayesiana.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ajustar l칩gica si el modelo es CNN o MLP:\n",
        "        if model_class == MLPModel:\n",
        "            model = model_class(input_dim=config[\"WINDOW_SIZE\"] * len(features),\n",
        "                             hidden_dim=params['hidden_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "        elif model_class == CNNModel:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "        else:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], layer_dim=params['layer_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
        "        train_losses, val_losses = train_model(model, train_dataset, val_dataset, epochs=params['epochs'], optimizer=optimizer, criterion=criterion, device=device)\n",
        "\n",
        "        return {'loss': val_losses[-1], 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "\n",
        "    # Get the best model from the trials\n",
        "    best_trial = trials.best_trial\n",
        "    best_model = best_trial['result']['model'] # Get the best model\n",
        "\n",
        "\n",
        "    return {'best_params': best, 'best_model': best_model}"
      ],
      "metadata": {
        "id": "qTvtQcr9ddwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparametros del espacio de b칰squeda"
      ],
      "metadata": {
        "id": "e07rplY-Adsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_choices = [50, 100, 150]\n",
        "hidden_dim_choices = [32, 64, 128]\n",
        "optimizer_choices = [optim.Adam, optim.RMSprop, optim.SGD]\n",
        "layer_dim_choices = [1, 2, 3]"
      ],
      "metadata": {
        "id": "l32P0EC2qHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 MLP"
      ],
      "metadata": {
        "id": "EfJFd6rXd1hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de b칰squeda para MLP\n",
        "# space_mlp = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimizaci칩n bayesiana para MLP\n",
        "# best_model = bayesian_optimization(MLPModel, train_loader, val_loader, space_mlp, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k1972LeeeOJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 CNN"
      ],
      "metadata": {
        "id": "T6G7tnlid46C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de b칰squeda para CNN\n",
        "# space_cnn = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimizaci칩n bayesiana para CNN\n",
        "# best_model = bayesian_optimization(CNNModel, train_loader, val_loader, space_cnn, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "azWrt01efI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 RNN"
      ],
      "metadata": {
        "id": "BV65Z64qd6sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de b칰squeda para RNN\n",
        "# space_rnn = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'layer_dim': hp.choice('layer_dim', layer_dim_choices),\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimizaci칩n bayesiana para RNN\n",
        "# best_model = bayesian_optimization(RNNModel, train_loader, val_loader, space_rnn, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "C-aXiElTfMZr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 LSTM"
      ],
      "metadata": {
        "id": "Ka4QI_0Wd8Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de b칰squeda para LSTM\n",
        "# space_lstm = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'layer_dim': hp.choice('layer_dim', layer_dim_choices),  # N칰mero de capas LSTM\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimizaci칩n bayesiana para LSTM\n",
        "# best_model = bayesian_optimization(LSTMModel, train_loader, val_loader, space_lstm, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "wgCEykClfPNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 GRU"
      ],
      "metadata": {
        "id": "eHT0fwu_d-X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el espacio de b칰squeda para GRU\n",
        "space_gru = {\n",
        "    'epochs': hp.choice('epochs', epochs_choices),\n",
        "    'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "    'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "    'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "    'layer_dim': hp.choice('layer_dim', layer_dim_choices),  # N칰mero de capas GRU\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "}\n",
        "\n",
        "# Realiza la optimizaci칩n bayesiana para GRU\n",
        "best_model = bayesian_optimization(GRUModel, train_loader, val_loader, space_gru, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "yXkRx_iffSDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejores hiperp치rametros"
      ],
      "metadata": {
        "id": "i_FI_b02AmAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime los mejores par치metros y el mejor modelo\n",
        "print(\"Mejores par치metros:\")\n",
        "print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\")\n",
        "print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\")\n",
        "print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\")\n",
        "print(f\"lr: {best_model['best_params']['lr']}\")\n",
        "print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\")\n",
        "if 'layer_dim' in best_model['best_params']:\n",
        "  print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\")\n",
        "print(\"Mejor modelo:\", best_model['best_model'])"
      ],
      "metadata": {
        "id": "BfbFkN59Mks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Predicci칩n en el Test Set"
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df.drop(columns=[target]), test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])\n",
        "\n",
        "test_sequences = []\n",
        "ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(config[\"WINDOW_SIZE\"])\n",
        "    if len(prev_dates) == config[\"WINDOW_SIZE\"]:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        ids.append(row['id'])\n",
        "\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Entrenar Modelo Final y Predecir"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_trained = copy.deepcopy(best_model['best_model']).to(DEVICE)"
      ],
      "metadata": {
        "id": "YDWNB-DX7m9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preparar la data completa para el entrenamiento\n",
        "train_df_full[features] = scaler_features.transform(train_df_full[features])\n",
        "train_df_full[target] = scaler_target.transform(train_df_full[[target]])\n",
        "\n",
        "# 2. Preparar la secuencia de la data completa\n",
        "train_sequences_full, train_labels_full = create_sequences(train_df_full, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "\n",
        "# 3. Crear dataset y dataloader\n",
        "train_dataset_full = DengueDataset(train_sequences_full, train_labels_full)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "xXizf831_oF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Entrenar con todo el dataset\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optimizer_choices[best_model['best_params']['optimizer']](best_model_trained.parameters(), lr=best_model['best_params']['lr'])\n",
        "\n",
        "print(\"\\n游대 Reentrenando con todo el dataset (train + val)...\\n\")\n",
        "epochs_retrain = 50\n",
        "for epoch in range(epochs_retrain):\n",
        "  best_model_trained.train()\n",
        "  train_loss = 0\n",
        "  for x_batch, y_batch in train_loader_full:\n",
        "    x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(best_model_trained(x_batch), y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "  print(f'Epoch {epoch+1}/{epochs_retrain}, Train Loss: {(train_loss/len(train_loader_full)):.4f}')"
      ],
      "metadata": {
        "id": "mwTir67aBfXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar dataset de prueba\n",
        "test_dataset = DengueTestDataset(test_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar predicciones\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for X in test_loader:\n",
        "        X = X.to(DEVICE)\n",
        "        output = best_model_trained(X)\n",
        "        predictions.append(output.cpu().numpy())\n",
        "predictions = np.concatenate(predictions)\n",
        "predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Preparar submission\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "print(f'Submission guardado en submission.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "0-bQaQwJd7b9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}